
\begin{alphaparts}
    \questionpart %----------------A-------------------------------
    La comunità di agenti si presenta sotto forma del seguente grafo:
    \ctikzfig{grafo_es3}
    
    Dal momento che quest'ultimo è fortemente connesso, la dinamica delle opinioni raggiunge il consenso:
    \[x(t) \xrightarrow[]{t\to \infty}(\pi'x(0))\mathbbm{1}.\]
    Poiché consideriamo \(x(0)\) come un vettore di variabili aleatorie indipendenti con le varianze date, allora esso avrà una matrice di varianza-covarianza così composta:
    \[VarCov(x(0)):= V= \begin{bmatrix}
        0.1 & 0 & 0 & 0 \\
        0 & 0.3 & 0 & 0\\
        0 & 0 & 0.2 & 0\\
        0 & 0 & 0 & 0.2
    \end{bmatrix}\]
    Pertanto, utilizzando la formula per la varianza di un prodotto scalare di una V.A. con un vettore costante, si ottiene:
    \[Var(\pi'x(0)) = \pi' V \pi.\]
    Si procede quindi con il calcolo di \(\pi\). Essendo il grafo fortemente connesso abbiamo:
    \[\pi = \frac{\omega}{\mathbbm{1}'\omega}\]
    pertanto:
    \[Var(\pi'x(0)) \sim 0.0519\]

    \questionpart %----------------B----------------------
    Sia \(V\) la matrice di varianza-covarianza di una comunità. La varianza del consenso \(\gamma\), se raggiunto, sarà data dalla formula:
    \[Var(\gamma) = \pi' V \pi\]
    dove \(\pi\) è la distribuzione stazionaria del grafo che connette la comunità. Costruire un grafo che minimizza la varianza del consenso (e quindi l'errore di misura nel caso di dinamiche di opinioni) significa trovare una \(\pi\) che risolve il seguente problema di minimo:
    \begin{align*}
        \text{minimize:} \quad & \pi'V\pi \\
        \text{subject to:} \quad &\mathbbm{1}'\pi = 1 \\
        & \pi \geq  0
    \end{align*}

    Risolvendo il problema senza considerare il constraint di non negatività si ottiene la funzione lagrangiana:
    \[\mathcal{L}(\pi, \lambda) = \pi'V\pi + \lambda(\mathbbm{1}'\pi- 1)\]
    e risolvendo il sistema:
    \begin{equation*}
        \begin{cases} \nabla_{\pi} \mathcal{L} = 2V\pi + \lambda \mathbbm{1} = 0 \\
        \nabla_{\lambda} \mathcal{L} = \mathbbm{1}'\pi- 1 = 0\end{cases} 
    \end{equation*}
    si trova la soluzione:
    \begin{equation}\label{optimal_pi}
        \pi^* = \frac{1}{\mathbbm{1}'V\mathbbm{1}} V^{- 1} \mathbbm{1}.
    \end{equation}
    Osserviamo che, poiché \(V\) in quanto matrice di varianza-covarianza, è semidefinita positiva, necessariamente \(\pi^*\) è un vettore non negativo, pertanto il constraint di non negatività è automaticamente soddisfatto.
    Osserviamo inoltre che, nel caso in cui \(V\) sia una matrice diagonale, la soluzione \(\pi^*\) trovata è tale per cui viene dato più peso ai nodi che hanno meno varianza: questo ha senso dal momento che se si considera la varianza come "errore di misura" allora per minimizzare questo errore al consenso bisogna dare più centralità ai nodi "esperti" ossia quelli con varianza bassa.

    \questionpart %-----------C------------------------------------

    Consideriamo il nuovo grafo con i self-loop e definiamo le quantità:
    \begin{gather*}
        \overline{W}= diag(\alpha)+ W\\
        \overline{\omega}= \omega+ \alpha\\
        \overline{D}= diag(\overline{\omega})
    \end{gather*}
    Dove \(\overline{W}\) e \(\overline{\omega}\) sono rispettivamente la matrice di adiacenza e il vettore degli out-degree del nuovo grafo. Calcoliamo ora la matrice stocastica \(\overline{P}\) del nuovo grafo:
    \begin{gather*}
        \overline{P}= \overline{D}^{-1}\overline{W}=\left(diag(\alpha)+ diag(\omega)\right)^{-1}\left(diag(\alpha)+ W\right)=\\
        =\left(diag(\alpha)+ diag(\omega)\right)^{-1}\left(diag(\alpha)+ diag(\omega)P\right)=\\
        = \left(diag(\alpha)+ diag(\omega)\right)^{-1}diag(\omega)P + \left(diag(\alpha)+ diag(\omega)\right)^{-1}diag(\alpha)
    \end{gather*} 
    Poiché \(\frac{\alpha}{\alpha+ \omega}= 1- \frac{\omega}{\alpha+ \omega}\), se definiamo il vettore \(a= \frac{\omega}{\alpha+ \omega}\), allora dai precedenti calcoli si ottiene\footnote{Con questa notazione si intende dire che \(a_i = \frac{\omega_i}{\alpha_i + \omega_i}\) pertanto \(diag(a) = diag(\frac{\omega}{\alpha+ \omega}) = \left(diag(\alpha)+ diag(\omega)\right)^{-1}diag(\omega) \).}:
    \[\overline{P}= diag(a)P+ I- diag(a).\]
    Osserviamo a questo punto che se definiamo 
    \[\overline{\pi} = diag(a)^{-1}\pi,\]
    dove \(\pi\) è la distribuzione stazionaria del grafo senza self loop, allora segue che:
    \begin{gather*}
        \overline{P}'\overline{\pi}=\left(diag(a)P+ I- diag(a)\right)'diag(a)^{-1}\pi = \\
        = \left(P'diag(a)+ I- diag(a)\right)diag(a)^{-1}\pi= \\
        = diag(a)^{-1}\pi- \pi+ P'\pi = \overline{\pi}.
    \end{gather*}
    Pertanto \(\overline{\pi}\) è una misura stazionaria del nuovo grafo. Affinché \(\overline{\pi}\) diventi una distribuzione, bisogna normalizzarla. Quindi:
    \[\hat{\pi} = \frac{\mathbbm{1}'\omega}{\mathbbm{1}'\alpha+\mathbbm{1}'\omega}\overline{\pi} = \frac{\alpha+ \omega}{\mathbbm{1}'\alpha+\mathbbm{1}'\omega}\]
    è effettivamente una distribuzione stazionaria di \(\overline{P}\). Utilizzando ora \ref{optimal_pi} impostiamo l'equazione per trovare il valore ottimale di \(\alpha\):
    \[\hat{\pi}(\alpha) = \frac{\alpha+ \omega}{\mathbbm{1}'\alpha+\mathbbm{1}'\omega} = \frac{1}{\mathbbm{1}'V^{-1}\mathbbm{1}}V^{-1}\mathbbm{1}. \]
    poiché
    \[V^{-1} = \begin{bmatrix}
        10 & 0 & 0 & 0 \\
        0 & \frac{10}{3} & 0 & 0 \\
        0 & 0 & 5 & 0\\
        0 & 0 & 0 & 5
    \end{bmatrix}, \quad\quad \mathbbm{1}'V^{-1}\mathbbm{1} = \frac{70}{3}, \quad\quad \mathbbm{1}'\omega = 9 \]
    si ottiene il sistema:
    \[\begin{cases}
        \alpha_1 + 2 = \left(\frac{27+ 3(\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4)}{70}\right)10\\
        \alpha_2 + 2 = \left(\frac{27+ 3(\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4)}{70}\right) \frac{10}{3}\\
        \alpha_3+ 2 = \left(\frac{27+ 3(\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4)}{70}\right) 5\\
        \alpha_4 + 3 = \left(\frac{27+ 3(\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4)}{70}\right) 5
    \end{cases}\]

    da cui si ottengono le soluzioni:
    \[\alpha^*(h) = \begin{pmatrix}
        2h+ 4 \\ \frac{2}{3}h \\ h+ 1 \\ h
    \end{pmatrix}\]
    Tra queste, quella che minimizza \(\mathbbm{1}'\alpha\), è
    \[\alpha^* = \begin{pmatrix}
        4 \\ 0 \\ 1 \\ 0
    \end{pmatrix}\] 

    \questionpart %-------------d------------------------------------------
    Analogamente al punto (b) calcoliamo la varianza del consenso come varianza di un prodotto scalare tra un vettore costante e un vettore aleatorio. Questa volta \(VarCov(x(0))= A\), pertanto:
    \[Var(\pi'x(0)) = \pi'A\pi \sim 0.1025\]

    \questionpart %--------------e-----------------------------------------
    Utilizzando lo stesso procedimento del punto (c) impostiamo l'equazione per trovare il valore ottimale di \(\alpha\), questa volta con una matrice di varianza covarianza diversa:
    \[\hat{\pi}(\alpha) = \frac{\alpha+ \omega}{\mathbbm{1}'\alpha+\mathbbm{1}'\omega} = \frac{1}{\mathbbm{1}'A^{-1}\mathbbm{1}}A^{-1}\mathbbm{1}. \]
    poiché
    \[\frac{1}{\mathbbm{1}'A^{-1}\mathbbm{1}}A^{-1}\mathbbm{1} = \begin{pmatrix}
        1/4 \\ 1/4 \\ 1/4 \\ 1/4
    \end{pmatrix}\]
    si ottiene il sistema:
    \[
        \begin{cases}
            \alpha_1 + 2 = (\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4 + 9) \frac{1}{4} \\
            \alpha_2 + 2 = (\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4 + 9) \frac{1}{4} \\
            \alpha_3 + 2 = (\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4 + 9) \frac{1}{4} \\
            \alpha_4 + 3 = (\alpha_1 + \alpha_2 + \alpha_3 + \alpha_4 + 9) \frac{1}{4}
        \end{cases}\]
        da cui si ottengono le soluzioni:
        \[
            \alpha= \begin{pmatrix}
                1+ h \\ 1+ h \\ 1+ h \\ h
            \end{pmatrix}.\]
        Tra queste, quella che minimizza la quantità \(\mathbbm{1}'\alpha\) è pari a:
        \[\alpha^* = \begin{pmatrix}
            1 \\ 1 \\ 1 \\ 0
        \end{pmatrix}\]


    
\end{alphaparts}
